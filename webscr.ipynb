{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: password=R@is7140144078135\n",
      "env: email=baigrais13@gmail.com\n"
     ]
    }
   ],
   "source": [
    "%env password = R@is7140144078135\n",
    "%set_env email = baigrais13@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R@is7140144078135'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = os.environ.get(\"email\")\n",
    "password = os.environ.get(\"password\")\n",
    "\n",
    "password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(options=options, service=service)\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "driver.maximize_window()\n",
    "time.sleep(.50)\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "username.send_keys(email) #enteremail\n",
    "time.sleep(1)\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "time.sleep(1)\n",
    "pword.send_keys(password)       #passw\n",
    "time.sleep(1) \n",
    "driver.find_element(By.XPATH, \"/html/body/div/main/div[2]/div[1]/form/div[3]/button\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://www.linkedin.com/search/results/companies/?companyHqGeo=%5B%22102221843%22%2C%22103644278%22%5D&companySize=%5B%22B%22%2C%22C%22%2C%22D%22%5D&keywords=azure%20data%20engineer&origin=FACETED_SEARCH&searchId=8531d094-0a6a-496e-b334-4852b3f8ca67&sid=Jn.\"\n",
    "\"https://www.linkedin.com/search/results/companies/?companyHqGeo=%5B%22102221843%22%2C%22103644278%22%5D&companySize=%5B%22B%22%2C%22C%22%2C%22D%22%5D&keywords=azure%20data%20engineer&origin=FACETED_SEARCH&page=2&searchId=8531d094-0a6a-496e-b334-4852b3f8ca67&sid=vuu\"\n",
    "\"https://www.linkedin.com/search/results/companies/?companyHqGeo=%5B%22102221843%22%2C%22103644278%22%5D&companySize=%5B%22B%22%2C%22C%22%2C%22D%22%5D&keywords=azure%20data%20engineer&origin=FACETED_SEARCH&page=3&searchId=8531d094-0a6a-496e-b334-4852b3f8ca67&sid=%3ASn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.linkedin.com/search/results/companies/?companyHqGeo=%5B%22102221843%22%2C%22103644278%22%5D&companySize=%5B%22B%22%2C%22C%22%2C%22D%22%5D&keywords=azure%20data%20engineer&origin=FACETED_SEARCH&searchId=8531d094-0a6a-496e-b334-4852b3f8ca67&sid=Jn.\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"}\n  (Session info: chrome=120.0.6099.217); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x006C6EE3+174339]\n\t(No symbol) [0x005F0A51]\n\t(No symbol) [0x00306FF6]\n\t(No symbol) [0x00339876]\n\t(No symbol) [0x00339C2C]\n\t(No symbol) [0x0036BD42]\n\t(No symbol) [0x00357054]\n\t(No symbol) [0x0036A104]\n\t(No symbol) [0x00356DA6]\n\t(No symbol) [0x00331034]\n\t(No symbol) [0x00331F8D]\n\tGetHandleVerifier [0x00764B1C+820540]\n\tsqlite3_dbdata_init [0x008253EE+653550]\n\tsqlite3_dbdata_init [0x00824E09+652041]\n\tsqlite3_dbdata_init [0x008197CC+605388]\n\tsqlite3_dbdata_init [0x00825D9B+656027]\n\t(No symbol) [0x005FFE6C]\n\t(No symbol) [0x005F83B8]\n\t(No symbol) [0x005F84DD]\n\t(No symbol) [0x005E5818]\n\tBaseThreadInitThunk [0x7735FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x776D7C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x776D7C3E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# /html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[2]/div/div[2]/div/button[2]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclick()\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:742\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    739\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    740\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:348\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"}\n  (Session info: chrome=120.0.6099.217); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x006C6EE3+174339]\n\t(No symbol) [0x005F0A51]\n\t(No symbol) [0x00306FF6]\n\t(No symbol) [0x00339876]\n\t(No symbol) [0x00339C2C]\n\t(No symbol) [0x0036BD42]\n\t(No symbol) [0x00357054]\n\t(No symbol) [0x0036A104]\n\t(No symbol) [0x00356DA6]\n\t(No symbol) [0x00331034]\n\t(No symbol) [0x00331F8D]\n\tGetHandleVerifier [0x00764B1C+820540]\n\tsqlite3_dbdata_init [0x008253EE+653550]\n\tsqlite3_dbdata_init [0x00824E09+652041]\n\tsqlite3_dbdata_init [0x008197CC+605388]\n\tsqlite3_dbdata_init [0x00825D9B+656027]\n\t(No symbol) [0x005FFE6C]\n\t(No symbol) [0x005F83B8]\n\t(No symbol) [0x005F84DD]\n\t(No symbol) [0x005E5818]\n\tBaseThreadInitThunk [0x7735FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x776D7C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x776D7C3E+238]\n"
     ]
    }
   ],
   "source": [
    "# /html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[2]/div/div[2]/div/button[2]\n",
    "driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the html page of the main page 1 having the names of companies with the given filters\n",
    "main_page1_html = driver.page_source\n",
    "\n",
    "# Creating the soup of first page \n",
    "soup = BeautifulSoup(main_page1_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.linkedin.com/company/datatalentadvisors/',\n",
       " 'https://www.linkedin.com/company/data-code-mx/',\n",
       " 'https://www.linkedin.com/company/data-management-services-inc-/',\n",
       " 'https://www.linkedin.com/company/data-engineering-team/',\n",
       " 'https://www.linkedin.com/company/capital-techsearch/',\n",
       " 'https://www.linkedin.com/company/gama-1-technologies/',\n",
       " 'https://www.linkedin.com/company/sourcecoders/',\n",
       " 'https://www.linkedin.com/company/softcomsystems-inc/',\n",
       " 'https://www.linkedin.com/company/snowrelic/',\n",
       " 'https://www.linkedin.com/company/marcusdonald/']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing the soup to get the links of all companies and next page link.\n",
    "\n",
    "# Main tag having the li tags of each company on\n",
    "main_div_tag = soup.find(\"div\", class_=\"search-results-container\")  #id=\"9rvnjUBgQRaOgxrpEzk3+w==\"\n",
    "\n",
    "# List of all the \"li\" tags which is having the url for each company page\n",
    "list_li_tags = main_div_tag.find_all(\"li\", class_=\"reusable-search__result-container\")\n",
    "\n",
    "# List of all companies links from the respective page. Here the respective page means the page this is currently loading/in process\n",
    "list_of_comp_links = []\n",
    "\n",
    "# Parsing through the tag and fetch the links of the companies\n",
    "for li in list_li_tags:\n",
    "    list_of_comp_links.append(li.find(\"a\")['href'])\n",
    "\n",
    "list_of_comp_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About comp links 10\n",
      "People comp links ['https://www.linkedin.com/company/datatalentadvisors//people/', 'https://www.linkedin.com/company/data-code-mx//people/', 'https://www.linkedin.com/company/data-management-services-inc-//people/', 'https://www.linkedin.com/company/data-engineering-team//people/', 'https://www.linkedin.com/company/capital-techsearch//people/', 'https://www.linkedin.com/company/gama-1-technologies//people/', 'https://www.linkedin.com/company/sourcecoders//people/', 'https://www.linkedin.com/company/softcomsystems-inc//people/', 'https://www.linkedin.com/company/snowrelic//people/', 'https://www.linkedin.com/company/marcusdonald//people/']\n"
     ]
    }
   ],
   "source": [
    "# This list contains all the about pages of all companies from respective page\n",
    "comp_about_page_links = []\n",
    "\n",
    "# This list contains all the People page of all companies from the respective page\n",
    "comp_people_page_links = []\n",
    "\n",
    "# Getting all the \n",
    "for comp in list_of_comp_links:\n",
    "\n",
    "    # About page of the company\n",
    "    comp_about_page = comp + \"/about/\"\n",
    "    comp_about_page_links.append(comp_about_page)\n",
    "\n",
    "    # People or employees page of the company\n",
    "    comp_people_page = comp + \"/people/\"\n",
    "    comp_people_page_links.append(comp_people_page)\n",
    "\n",
    "print(\"About comp links\", len(comp_about_page_links))\n",
    "print(\"People comp links\", comp_people_page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'manager'\n",
    "people_page = f\"{comp_people_page_links[0]}?keywords={keyword}\"\n",
    "\n",
    "print(people_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_page1=comp_about_page_links[0]\n",
    "about_page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the len of any of the list values in about_page_dict is null then we will add None in it\n",
    "def checkNull(about_page_dict):\n",
    "    if len(about_page_dict[\"Website\"])==0:\n",
    "        about_page_dict[\"Website\"].append(None)\n",
    "    elif len(about_page_dict[\"Phone\"])==0:\n",
    "        about_page_dict[\"Phone\"].append(None)\n",
    "    elif len(about_page_dict[\"Industry\"])==0:\n",
    "        about_page_dict[\"Industry\"].append(None)\n",
    "    elif len(about_page_dict[\"Company size\"])==0:\n",
    "        about_page_dict[\"Company size\"].append(None)\n",
    "    elif len(about_page_dict[\"Headquarters\"])==0:\n",
    "        about_page_dict[\"Headquarters\"].append(None)\n",
    "    elif len(about_page_dict[\"Founded\"])==0:\n",
    "        about_page_dict[\"Founded\"].append(None)\n",
    "    elif len(about_page_dict[\"Specialties\"])==0:\n",
    "        about_page_dict[\"Specialties\"].append(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting into the about page of the company\n",
    "def AboutCompData(comp_aboutPage_soup):\n",
    "    \n",
    "\n",
    "    # Parsing the about page\n",
    "    # About page div tag\n",
    "    about_page_Overview_div_tag = comp_aboutPage_soup.find(\"div\", class_=\"mb6\")  #id=\"ember49\"\n",
    "    about_page_dl_tag = about_page_Overview_div_tag.find(\"dl\", class_=\"overflow-hidden\")\n",
    "\n",
    "    # Titles from the about page line website, industry, comp size, headquarters, specialities etc\n",
    "    dt_tag_list = about_page_dl_tag.find_all(\"dt\", class_=\"mb1 text-heading-medium\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Tag names that will be the column names in the final file\n",
    "    \n",
    "    for tag in dt_tag_list:\n",
    "        # about_page_dict[\"Columns_names\"].append(tag.text.strip())\n",
    "        if tag.text.strip() == \"Website\":\n",
    "            tg = tag.find_next_sibling(\"dd\")\n",
    "            website= tg.find(\"a\")['href']\n",
    "            if website:\n",
    "                comp_web = website\n",
    "            # about_page_dict[\"Website\"].append(website)\n",
    "            # print(website)\n",
    "\n",
    "        elif tag.text.strip() == \"Phone\":\n",
    "            Phone = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Phone:\n",
    "                comp_Phone = Phone\n",
    "            # about_page_dict[\"Phone\"].append(Phone)        \n",
    "\n",
    "        elif tag.text.strip() == \"Industry\":\n",
    "            Industry = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Industry:\n",
    "                comp_Industry = Industry\n",
    "            # about_page_dict[\"Industry\"].append(Industry)\n",
    "            # print(Industry)\n",
    "\n",
    "        elif tag.text.strip() == \"Company size\":\n",
    "            Company_size = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Company_size:\n",
    "                comp_Company_size = Company_size\n",
    "            # about_page_dict[\"Company size\"].append(Company_size)\n",
    "            # print(Company_size)\n",
    "\n",
    "        elif tag.text.strip() == \"Headquarters\":\n",
    "            Headquarters = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Headquarters:\n",
    "                comp_Headquarters = Headquarters\n",
    "            # about_page_dict[\"Headquarters\"].append(Headquarters)\n",
    "            # print(Headquarters)\n",
    "\n",
    "        elif tag.text.strip() == \"Founded\":\n",
    "            Founded = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Founded:\n",
    "                comp_Founded = Founded\n",
    "            # about_page_dict[\"Founded\"].append(Founded)\n",
    "            # print(Headquarters)\n",
    "\n",
    "        elif tag.text.strip() == \"Specialties\":\n",
    "            Specialties = tag.find_next_sibling(\"dd\").text.strip()\n",
    "            if Specialties:\n",
    "                comp_Specialties = Specialties\n",
    "            # about_page_dict[\"Specialties\"].append(Specialties)\n",
    "            # print(Specialties)\n",
    "        about_page_dict = {\n",
    "        \"Website\":comp_web,\n",
    "        \"Phone\":comp_Phone,\n",
    "        \"Industry\":comp_Industry,\n",
    "        \"Company size\":comp_Company_size,\n",
    "        \"Headquarters\":comp_Headquarters,\n",
    "        \"Founded\":comp_Founded,\n",
    "        \"Specialties\":comp_Specialties\n",
    "    }\n",
    "\n",
    "    # checkNull(about_page_dict)\n",
    "\n",
    "\n",
    "    return about_page_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def About_Comp_Data(comp_aboutPage_soup):\n",
    "    sections = comp_aboutPage_soup.find_all(\"section\")\n",
    "    \n",
    "    for section in sections:\n",
    "        h2_tag = section.find(\"h2\")\n",
    "        if h2_tag:\n",
    "            \n",
    "            if h2_tag.text.strip() == \"Overview\":\n",
    "                \n",
    "                about_page_dl_tag = section.find(\"dl\", class_=\"overflow-hidden\")\n",
    "                \n",
    "                # Titles from the about page line website, industry, comp size, headquarters, specialities etc\n",
    "                dt_tag_list = about_page_dl_tag.find_all(\"dt\", class_=\"mb1 text-heading-medium\")\n",
    "                print(\"found the list of dt tags\")\n",
    "                list_dict_aboutpate = []\n",
    "                comp_name = \"RAis\" #section.find(\"div\", class_=\"block mt2\").find(\"h1\").find(\"span\").text.strip()\n",
    "                Phone = Industry = Company_size = Headquarters = Founded = Specialties = None\n",
    "                for tag in dt_tag_list:\n",
    "                    # print(tag.text.strip())\n",
    "                    com_ind = tag.text.strip()\n",
    "                    if com_ind == \"Website\":\n",
    "                        Website_tag = tag.find_next_sibling(\"dd\")\n",
    "                        Website = Website_tag.find(\"a\")['href']\n",
    "                        # print(\"Website\", Website)\n",
    "                    if com_ind == \"Phone\":\n",
    "                        Phone_tag1 = tag.find_next_sibling(\"dd\")\n",
    "                        Phone_tag = Phone_tag1.find(\"a\", class_=\"link-without-visited-state ember-view\")\n",
    "                        Phone = Phone_tag.find(\"span\", class_=\"link-without-visited-state\").text.strip()\n",
    "                        # print(\"Phone\", Phone)\n",
    "                    if com_ind ==\"Industry\":\n",
    "                        Industry = tag.find_next_sibling(\"dd\").text.strip()\n",
    "                        # print(\"Industry\", Industry)\n",
    "                    if com_ind ==\"Company size\":\n",
    "                        Company_size = tag.find_next_sibling(\"dd\").text.strip()\n",
    "                        # print(\"Company_size\", Company_size)\n",
    "                    if com_ind ==\"Headquarters\":\n",
    "                        Headquarters = tag.find_next_sibling(\"dd\").text.strip()\n",
    "                        # print(\"Headquarters\", Headquarters)\n",
    "                    if com_ind ==\"Founded\":\n",
    "                        Founded = tag.find_next_sibling(\"dd\").text.strip()\n",
    "                        # print(\"Founded\", Founded)\n",
    "                    if com_ind ==\"Specialties\":\n",
    "                        Specialties = tag.find_next_sibling(\"dd\").text.strip()\n",
    "                        # print(\"Specialties\", Specialties)\n",
    "\n",
    "                about_page_dict = {\n",
    "                        # \"Company Name\":comp_name,\n",
    "                        \"Website\":Website,\n",
    "                        \"Phone\":Phone,\n",
    "                        \"Industry\":Industry,\n",
    "                        \"Company size\":Company_size,\n",
    "                        \"Headquarters\":Headquarters,\n",
    "                        \"Founded\":Founded,\n",
    "                        \"Specialties\":Specialties\n",
    "                }\n",
    "\n",
    "                # list_dict_aboutpate.append(about_page_dict)\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "                return about_page_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/company/datatalentadvisors//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/data-code-mx//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/data-management-services-inc-//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/data-engineering-team//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/capital-techsearch//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/gama-1-technologies//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/sourcecoders//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/softcomsystems-inc//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/snowrelic//about/\n",
      "found the list of dt tags\n",
      "https://www.linkedin.com/company/marcusdonald//about/\n",
      "found the list of dt tags\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Company size</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Specialties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.datatalentadvisors.com</td>\n",
       "      <td>None</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>11-50 employees</td>\n",
       "      <td>New York , NY</td>\n",
       "      <td>2017</td>\n",
       "      <td>Big Data, Analytics , Talent Acquistion, Headh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.datacode.mx</td>\n",
       "      <td>+52 55 3985 1756</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>51-200 employees</td>\n",
       "      <td>None</td>\n",
       "      <td>2016</td>\n",
       "      <td>Big Data, Data Warehouse, Integration, Busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.dms2.com</td>\n",
       "      <td>908-834-9009</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>2-10 employees</td>\n",
       "      <td>Somerville, New Jersey</td>\n",
       "      <td>1991</td>\n",
       "      <td>Network Infrastructure, Wireless Services, VoI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://DataEngineering.team</td>\n",
       "      <td>(954) 651-2475</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>11-50 employees</td>\n",
       "      <td>Miami, Florida</td>\n",
       "      <td>None</td>\n",
       "      <td>Big Data, Data Lakes, Data Pipelines, AWS, Azu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://capitaltechsearch.com</td>\n",
       "      <td>804.282.8788</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>51-200 employees</td>\n",
       "      <td>Richmond, Virginia</td>\n",
       "      <td>2001</td>\n",
       "      <td>Application Development, Infrastructure, Data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.gama1tech.com/</td>\n",
       "      <td>301-982-4262</td>\n",
       "      <td>Information Technology &amp; Services</td>\n",
       "      <td>51-200 employees</td>\n",
       "      <td>Greenbelt, Maryland</td>\n",
       "      <td>2006</td>\n",
       "      <td>Program Management, Infrastructure Services, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://sourcecoders.io</td>\n",
       "      <td>None</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>11-50 employees</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>2016</td>\n",
       "      <td>Technical Recruiting, Software Engineering Rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://www.softcomsystems.com</td>\n",
       "      <td>None</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>51-200 employees</td>\n",
       "      <td>Princeton, New Jersey</td>\n",
       "      <td>1997</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://www.snowrelic.com</td>\n",
       "      <td>None</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>11-50 employees</td>\n",
       "      <td>Reston, Virginia</td>\n",
       "      <td>2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>www.marcusdonald.com</td>\n",
       "      <td>02033280400</td>\n",
       "      <td>Staffing and Recruiting</td>\n",
       "      <td>11-50 employees</td>\n",
       "      <td>Blackfriars, England</td>\n",
       "      <td>2005</td>\n",
       "      <td>Infrastructure, Networking, Information Securi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Website             Phone  \\\n",
       "0  http://www.datatalentadvisors.com              None   \n",
       "1             http://www.datacode.mx  +52 55 3985 1756   \n",
       "2                http://www.dms2.com      908-834-9009   \n",
       "3        http://DataEngineering.team    (954) 651-2475   \n",
       "4      https://capitaltechsearch.com      804.282.8788   \n",
       "5          http://www.gama1tech.com/      301-982-4262   \n",
       "6            https://sourcecoders.io              None   \n",
       "7      http://www.softcomsystems.com              None   \n",
       "8           http://www.snowrelic.com              None   \n",
       "9               www.marcusdonald.com       02033280400   \n",
       "\n",
       "                            Industry      Company size  \\\n",
       "0            Staffing and Recruiting   11-50 employees   \n",
       "1      IT Services and IT Consulting  51-200 employees   \n",
       "2                 Telecommunications    2-10 employees   \n",
       "3      IT Services and IT Consulting   11-50 employees   \n",
       "4            Staffing and Recruiting  51-200 employees   \n",
       "5  Information Technology & Services  51-200 employees   \n",
       "6            Staffing and Recruiting   11-50 employees   \n",
       "7            Staffing and Recruiting  51-200 employees   \n",
       "8      IT Services and IT Consulting   11-50 employees   \n",
       "9            Staffing and Recruiting   11-50 employees   \n",
       "\n",
       "              Headquarters Founded  \\\n",
       "0            New York , NY    2017   \n",
       "1                     None    2016   \n",
       "2   Somerville, New Jersey    1991   \n",
       "3           Miami, Florida    None   \n",
       "4       Richmond, Virginia    2001   \n",
       "5      Greenbelt, Maryland    2006   \n",
       "6  Los Angeles, California    2016   \n",
       "7    Princeton, New Jersey    1997   \n",
       "8         Reston, Virginia    2021   \n",
       "9     Blackfriars, England    2005   \n",
       "\n",
       "                                         Specialties  \n",
       "0  Big Data, Analytics , Talent Acquistion, Headh...  \n",
       "1  Big Data, Data Warehouse, Integration, Busines...  \n",
       "2  Network Infrastructure, Wireless Services, VoI...  \n",
       "3  Big Data, Data Lakes, Data Pipelines, AWS, Azu...  \n",
       "4  Application Development, Infrastructure, Data,...  \n",
       "5  Program Management, Infrastructure Services, I...  \n",
       "6  Technical Recruiting, Software Engineering Rec...  \n",
       "7                                               None  \n",
       "8                                               None  \n",
       "9  Infrastructure, Networking, Information Securi...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# about_page1=comp_about_page_links[0]\n",
    "comp_details_list = []\n",
    "for about_page in comp_about_page_links:\n",
    "    print(about_page)\n",
    "    driver.get(about_page)\n",
    "    time.sleep(2)\n",
    "    # soup for the about page\n",
    "    comp_aboutPage_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(2)\n",
    "    details_dict = About_Comp_Data(comp_aboutPage_soup)\n",
    "    comp_details_list.append(details_dict)\n",
    "\n",
    "df = pd.DataFrame(comp_details_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_ppl_page in comp_people_page_links[:5]:\n",
    "    print(comp_ppl_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/company/datatalentadvisors//people/\n",
      "https://www.linkedin.com/company/data-code-mx//people/\n",
      "https://www.linkedin.com/company/data-management-services-inc-//people/\n",
      "https://www.linkedin.com/company/data-engineering-team//people/\n",
      "https://www.linkedin.com/company/capital-techsearch//people/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>profile_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lena Hlushchuk</td>\n",
       "      <td>https://www.linkedin.com/in/lena-hlushchuk-094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalya Dychko</td>\n",
       "      <td>https://www.linkedin.com/in/natalya-dychko?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassul Fazelat</td>\n",
       "      <td>https://www.linkedin.com/in/rassulf?miniProfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lena Hlushchuk</td>\n",
       "      <td>https://www.linkedin.com/in/lena-hlushchuk-094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natalya Dychko</td>\n",
       "      <td>https://www.linkedin.com/in/natalya-dychko?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rassul Fazelat</td>\n",
       "      <td>https://www.linkedin.com/in/rassulf?miniProfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lena Hlushchuk</td>\n",
       "      <td>https://www.linkedin.com/in/lena-hlushchuk-094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lorie K. Smith-Gray</td>\n",
       "      <td>https://www.linkedin.com/in/loriek38?miniProfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ramesh Reddy</td>\n",
       "      <td>https://www.linkedin.com/in/ramesh-reddy-45064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David C.</td>\n",
       "      <td>https://www.linkedin.com/in/david-c-b295126?mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stacey Teplitzky</td>\n",
       "      <td>https://www.linkedin.com/in/stacey-teplitzky-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bruce Yoder</td>\n",
       "      <td>https://www.linkedin.com/in/bruce-yoder-b30ab0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Duncan Donohue</td>\n",
       "      <td>https://www.linkedin.com/in/duncan-donohue?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stacey Teplitzky</td>\n",
       "      <td>https://www.linkedin.com/in/stacey-teplitzky-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ramesh Reddy</td>\n",
       "      <td>https://www.linkedin.com/in/ramesh-reddy-45064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>David C.</td>\n",
       "      <td>https://www.linkedin.com/in/david-c-b295126?mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lorie K. Smith-Gray</td>\n",
       "      <td>https://www.linkedin.com/in/loriek38?miniProfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Barthe van Doorn</td>\n",
       "      <td>https://www.linkedin.com/in/barthevandoorn?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LinkedIn Member</td>\n",
       "      <td>https://www.linkedin.com/in/barthevandoorn?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LinkedIn Member</td>\n",
       "      <td>https://www.linkedin.com/in/barthevandoorn?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>David Ingram</td>\n",
       "      <td>https://www.linkedin.com/in/dingram?miniProfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brady Diggs</td>\n",
       "      <td>https://www.linkedin.com/in/bradydiggs?miniPro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Matthew Walker</td>\n",
       "      <td>https://www.linkedin.com/in/matthew-walker-rva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cindy Ingram</td>\n",
       "      <td>https://www.linkedin.com/in/cynthiaingram?mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Doug Ingram, PhD</td>\n",
       "      <td>https://www.linkedin.com/in/douglassingram?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Matthew Walker</td>\n",
       "      <td>https://www.linkedin.com/in/matthew-walker-rva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>David Ingram</td>\n",
       "      <td>https://www.linkedin.com/in/dingram?miniProfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Doug Ingram, PhD</td>\n",
       "      <td>https://www.linkedin.com/in/douglassingram?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Brady Diggs</td>\n",
       "      <td>https://www.linkedin.com/in/bradydiggs?miniPro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Matthew Walker</td>\n",
       "      <td>https://www.linkedin.com/in/matthew-walker-rva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Barthe van Doorn</td>\n",
       "      <td>https://www.linkedin.com/in/barthevandoorn?min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>David Ingram</td>\n",
       "      <td>https://www.linkedin.com/in/dingram?miniProfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Doug Ingram, PhD</td>\n",
       "      <td>https://www.linkedin.com/in/douglassingram?min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                       profile_link\n",
       "0        Lena Hlushchuk  https://www.linkedin.com/in/lena-hlushchuk-094...\n",
       "1        Natalya Dychko  https://www.linkedin.com/in/natalya-dychko?min...\n",
       "2        Rassul Fazelat  https://www.linkedin.com/in/rassulf?miniProfil...\n",
       "3        Lena Hlushchuk  https://www.linkedin.com/in/lena-hlushchuk-094...\n",
       "4        Natalya Dychko  https://www.linkedin.com/in/natalya-dychko?min...\n",
       "5        Rassul Fazelat  https://www.linkedin.com/in/rassulf?miniProfil...\n",
       "6        Lena Hlushchuk  https://www.linkedin.com/in/lena-hlushchuk-094...\n",
       "7   Lorie K. Smith-Gray  https://www.linkedin.com/in/loriek38?miniProfi...\n",
       "8          Ramesh Reddy  https://www.linkedin.com/in/ramesh-reddy-45064...\n",
       "9              David C.  https://www.linkedin.com/in/david-c-b295126?mi...\n",
       "10     Stacey Teplitzky  https://www.linkedin.com/in/stacey-teplitzky-1...\n",
       "11          Bruce Yoder  https://www.linkedin.com/in/bruce-yoder-b30ab0...\n",
       "12       Duncan Donohue  https://www.linkedin.com/in/duncan-donohue?min...\n",
       "13     Stacey Teplitzky  https://www.linkedin.com/in/stacey-teplitzky-1...\n",
       "14         Ramesh Reddy  https://www.linkedin.com/in/ramesh-reddy-45064...\n",
       "15             David C.  https://www.linkedin.com/in/david-c-b295126?mi...\n",
       "16  Lorie K. Smith-Gray  https://www.linkedin.com/in/loriek38?miniProfi...\n",
       "17     Barthe van Doorn  https://www.linkedin.com/in/barthevandoorn?min...\n",
       "18      LinkedIn Member  https://www.linkedin.com/in/barthevandoorn?min...\n",
       "19      LinkedIn Member  https://www.linkedin.com/in/barthevandoorn?min...\n",
       "20         David Ingram  https://www.linkedin.com/in/dingram?miniProfil...\n",
       "21          Brady Diggs  https://www.linkedin.com/in/bradydiggs?miniPro...\n",
       "22       Matthew Walker  https://www.linkedin.com/in/matthew-walker-rva...\n",
       "23         Cindy Ingram  https://www.linkedin.com/in/cynthiaingram?mini...\n",
       "24     Doug Ingram, PhD  https://www.linkedin.com/in/douglassingram?min...\n",
       "25       Matthew Walker  https://www.linkedin.com/in/matthew-walker-rva...\n",
       "26         David Ingram  https://www.linkedin.com/in/dingram?miniProfil...\n",
       "27     Doug Ingram, PhD  https://www.linkedin.com/in/douglassingram?min...\n",
       "28          Brady Diggs  https://www.linkedin.com/in/bradydiggs?miniPro...\n",
       "29       Matthew Walker  https://www.linkedin.com/in/matthew-walker-rva...\n",
       "30     Barthe van Doorn  https://www.linkedin.com/in/barthevandoorn?min...\n",
       "31         David Ingram  https://www.linkedin.com/in/dingram?miniProfil...\n",
       "32     Doug Ingram, PhD  https://www.linkedin.com/in/douglassingram?min..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_of_emp = []\n",
    "for comp_ppl_page in comp_people_page_links[:5]:\n",
    "    key_words = [\"manager\", \"Co-founder\",\"CEO\",  \"cto\", \"director\"]\n",
    "    print(comp_ppl_page)\n",
    "    \n",
    "    for word in key_words:\n",
    "        people_page = f\"{comp_ppl_page}?keywords={word}\"\n",
    "        driver.get(people_page)\n",
    "        time.sleep(4)\n",
    "        people_html = driver.page_source\n",
    "        # Creating a soup for the people page\n",
    "        people_html_soup = BeautifulSoup(people_html, 'html.parser')\n",
    "        time.sleep(1)\n",
    "        main_div_tag = people_html_soup.find(\"div\", class_=\"artdeco-card org-people-profile-card__card-spacing org-people__card-margin-bottom\")\n",
    "        if main_div_tag:\n",
    "            div_emp_tag = main_div_tag.find(\"ul\", class_=\"display-flex list-style-none flex-wrap\")\n",
    "            emp_list = div_emp_tag.find_all(\"li\", class_=\"grid grid__col--lg-8 block org-people-profile-card__profile-card-spacing\")\n",
    "            name = profile_link = None\n",
    "            for emp in emp_list:\n",
    "                div_tag = emp.find(\"div\", {\"class\":\"artdeco-entity-lockup__image artdeco-entity-lockup__image--type-circle ember-view\"})\n",
    "                prof_link_a_tag = div_tag.find(\"a\", class_=\"app-aware-link\")\n",
    "                name_div_tag = div_tag.find_next_sibling(\"div\", class_=\"artdeco-entity-lockup__content ember-view\")\n",
    "                div_nam = name_div_tag.find(\"div\", class_=\"artdeco-entity-lockup__title ember-view\")\n",
    "                pro_name = div_nam.find(\"div\", class_=\"ember-view lt-line-clamp lt-line-clamp--single-line org-people-profile-card__profile-title t-black\")\n",
    "                \n",
    "                if pro_name:\n",
    "                    name = pro_name.text.strip()\n",
    "                \n",
    "                if prof_link_a_tag:\n",
    "                    pro_link = prof_link_a_tag['href']\n",
    "                    profile_link = pro_link\n",
    "                dict = {\"name\":name, \"profile_link\":profile_link}\n",
    "                lst_of_emp.append(dict)\n",
    "\n",
    "df = pd.DataFrame(lst_of_emp)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_page_data(driver, profile_page_html):\n",
    "    soup = BeautifulSoup(profile_page_html, 'html.parser')\n",
    "    time.sleep(2)\n",
    "    # Parsing through each profile page html and extract the profile data\n",
    "    profile_data = {\n",
    "    \"Name\": [],\n",
    "    \"Profile Title\": [],\n",
    "    \"Job\": [],\n",
    "    \"Company Name\": [],\n",
    "    \"Email\": [],\n",
    "    \"Location\": []\n",
    "    }\n",
    "    #Fetching the name of the profile\n",
    "    name_tag = soup.find(\"h1\", class_='text-heading-xlarge inline t-24 v-align-middle break-words')\n",
    "    profile_data[\"Name\"].append(name_tag.text.strip())\n",
    "    print(f\"Fetchind data form {name_tag.text.strip()}\")\n",
    "\n",
    "    # Fetching the profile tag\n",
    "    profile_title_tag = soup.find(\"div\", class_='text-body-medium break-words')\n",
    "    profile_data[\"Profile Title\"].append(profile_title_tag.text.strip())\n",
    "    # print(\"Profile Title tag line\", profile_title_tag.text.strip())\n",
    "\n",
    "    # Fetching the job title of the profile\n",
    "    sections = soup.find_all(\"section\")\n",
    "    for section in sections:\n",
    "        div = section.find(\"div\", id=\"experience\")\n",
    "        if div:\n",
    "            div_exp_tag = section.find(\"div\",class_=\"pvs-list__outer-container\")\n",
    "            profile_job_tag = div_exp_tag.find(\"div\", class_='display-flex flex-wrap align-items-center full-height')\n",
    "            job = profile_job_tag.find('span', {\"aria-hidden\":\"true\"})\n",
    "            print(job.text.strip())\n",
    "            profile_job_company_tag = div_exp_tag.find(\"span\", class_='t-14 t-normal')\n",
    "            company_name_tag = profile_job_company_tag.find('span', {\"aria-hidden\": \"true\"})\n",
    "            company = company_name_tag.text.strip().split(\"\")[0]\n",
    "            profile_data[\"Job\"].append(job.text.strip())\n",
    "            profile_data[\"Company Name\"].append(company)\n",
    "\n",
    "    # print(\"Job title\", job.text.strip())\n",
    "\n",
    "    # Fetching the name of a company a profile holder working in\n",
    "    \"\"\"\n",
    "    sections = soup.find_all(\"section\")\n",
    "    for section in sections:\n",
    "        div = section.find(\"div\", id=\"experience\")\n",
    "        if div:\n",
    "            div_exp_tag = section.find(\"div\",class_=\"pvs-list__outer-container\")\n",
    "            profile_job_company_tag = div_exp_tag.find(\"span\", class_='t-14 t-normal')\n",
    "            company_name_tag = profile_job_company_tag.find('span', {\"aria-hidden\": \"true\"})\n",
    "            company = company_name_tag.text.strip().split(\"\")[0]\n",
    "            profile_data[\"Company Name\"].append(company)\n",
    "            # print(\"Company Name\", company)\n",
    "\n",
    "    \"\"\"\n",
    "    # Fetching the location of profile holder working\n",
    "    location = soup.find('span', class_='text-body-small inline t-black--light break-words')\n",
    "    profile_data[\"Location\"].append(location.text.strip())\n",
    "    # print(location.text.strip())\n",
    "\n",
    "    # Fetching the link for profile info page\n",
    "    profile_info_page_link = soup.find(\"a\", id='top-card-text-details-contact-info')\n",
    "    profile_contact_info_link = \"https://www.linkedin.com\" + profile_info_page_link['href']\n",
    "    driver.find_element(By.ID, \"top-card-text-details-contact-info\").click()\n",
    "    time.sleep(2)\n",
    "    driver.get(profile_contact_info_link)\n",
    "    time.sleep(1)\n",
    "    email_page = driver.page_source\n",
    "    soup = BeautifulSoup(email_page, 'html.parser')\n",
    "    div_tag = soup.find('a', {\"class\":\"bWCFkfQvJKVrOchJdgENAUvGnHahyLIMLqpA link-without-visited-state t-14\", \"target\":\"_blank\", \"rel\":\"noopener noreferrer\"})\n",
    "    if div_tag:\n",
    "        profile_data[\"Email\"].append(div_tag.text.strip())\n",
    "        # print(div_tag.text.strip())\n",
    "    else:\n",
    "        profile_data[\"Email\"].append(None)\n",
    "        # print(f\"email address for {name_tag.text.strip()} does not exists.\")\n",
    "    # print(profile_data)\n",
    "    # df = pd.DataFrame(profile_data)\n",
    "    # df.to_csv(\"Data/profile_data.csv\", index=False)\n",
    "    \n",
    "    with open(\"C:\\\\Users\\\\Aridian Technologies\\\\Desktop\\\\Office\\\\Desktop\\\\Data Scrappers\\\\Scraping_Companes_On_Linkedin\\\\Data\\\\linkedInData_New.csv\", mode='a', newline='') as csv_file:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        # If the file is empty, write the header\n",
    "        if csv_file.tell() == 0:\n",
    "            header = list(profile_data.keys())\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "        # Write data to the CSV file\n",
    "        for i in range(len(profile_data[\"Name\"])):\n",
    "            row_data = [profile_data[key][i] for key in profile_data]\n",
    "            csv_writer.writerow(row_data)\n",
    "    print(profile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Extract unique profile links for each unique name\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     unique_profile_links \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofile_link\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Print or process the unique name and corresponding profile links\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32mc:\\Users\\Aridian Technologies\\Desktop\\Office\\Desktop\\Data Scrappers\\Scraping_Companes_On_Linkedin\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "for name, group in df.groupby('name'):\n",
    "    # Extract unique profile links for each unique name\n",
    "    unique_profile_links = group['profile_link'].unique()\n",
    "    \n",
    "    # Print or process the unique name and corresponding profile links\n",
    "    print(f\"Name: {name}\")\n",
    "    for link in unique_profile_links:\n",
    "        pro_link = link\n",
    "        driver.get(pro_link)\n",
    "        time.sleep(1)\n",
    "        profile_html = driver.page_source\n",
    "        profile_page_data(driver, profile_html)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile_link</th>\n",
       "      <th>Website</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>http://www.datatalentadvisors.com</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Profile_link                            Website Email\n",
       "0         None  http://www.datatalentadvisors.com  None"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://www.linkedin.com/in/rassulf/overlay/contact-info/\")\n",
    "time.sleep(1)\n",
    "profile_html = driver.page_source\n",
    "soup = BeautifulSoup(profile_html, 'html.parser')\n",
    "lst_of_contacts = []\n",
    "info_sections = soup.find_all(\"section\")\n",
    "for section in info_sections:\n",
    "    h2_tag = section.find(\"h2\")\n",
    "    if h2_tag:\n",
    "        if h2_tag.text.strip() == \"Contact Info\":\n",
    "            # print(h2_tag.text.strip())\n",
    "            contact_div_tag = section.find(\"div\", class_=\"pv-profile-section__section-info section-info\")\n",
    "            contact_int_sections = contact_div_tag.find_all(\"section\", class_=\"pv-contact-info__contact-type\")\n",
    "            profile_link = website = Email = None\n",
    "            for sections in contact_int_sections:\n",
    "                profile_section = sections.find(\"h3\").text.strip() == \"Your Profile\"\n",
    "                if profile_section:\n",
    "                    profile_link = sections.find(\"div\", class_=\"pv-contact-info__ci-container t-14\").find(\"a\")['href']  \n",
    "\n",
    "                Web_section = sections.find(\"h3\").text.strip() == \"Website\"\n",
    "                if Web_section:\n",
    "                    website = sections.find(\"li\", class_=\"pv-contact-info__ci-container link t-14\").find(\"a\")['href']\n",
    "\n",
    "                Email_section = sections.find(\"h3\").text.strip() == \"Email\"\n",
    "                if Email_section:\n",
    "                    Email = sections.find(\"div\", class_=\"pv-contact-info__ci-container t-14\").find(\"a\").text.strip()\n",
    "                   \n",
    "\n",
    "            contact_dict = {\n",
    "            \"Profile_link\":profile_link,\n",
    "            \"Website\":website,\n",
    "            \"Email\":Email\n",
    "            }\n",
    "            lst_of_contacts.append(contact_dict)\n",
    "df =pd.DataFrame(lst_of_contacts)\n",
    "df        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
